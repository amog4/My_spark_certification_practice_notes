{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599724965204",
   "display_name": "Python 3.6.9 64-bit ('sparkit': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import sha1, rand\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('udf').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_split(x):\n",
    "  return x.split(\"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "manualSplitPythonUDF = spark.udf.register(\"manualSplitSQLUDF\", manual_split, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomDf = spark.range(100,1000*10).\\\n",
    "    withColumn('random_v', rand(seed=10).cast(\"string\") )\\\n",
    "    .withColumn(\"hash\", sha1(\"random_v\"))\\\n",
    "  .drop(\"random_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+-------------------+--------------------+\n| id|           random_v|                hash|\n+---+-------------------+--------------------+\n|100|0.41371264720975787|6f65a37773e1a173a...|\n|101| 0.7311719281896606|3c5f154011aa08151...|\n|102| 0.9031701155118229|28fc878d1f1942bd7...|\n|103|0.09430205113458567|9c54ebb1c11c92d4c...|\n|104|0.38340505276222947|46500976da0a25029...|\n|105| 0.5569246135523511|9d8e41374ef6637e2...|\n|106| 0.4977441406613893|29d9fa515d557568a...|\n|107| 0.2076666106201438|672da7735c57d6707...|\n|108| 0.9571919406508957|e6fb1c43359cff2d7...|\n|109| 0.7429395461204413|8e3f2b2dfae9a25aa...|\n|110| 0.3383362304807752|9b6f99aeded8d7460...|\n|111| 0.6701724731609291|455139cdd2f5c2459...|\n|112| 0.6417696089901257|38ee7efbe6c15078b...|\n|113| 0.7241109765059401|d81a35b183bf54307...|\n|114|0.34089575652338666|1bd6062a7dec0b5a8...|\n|115|0.24856531779931312|9b691311770dcc119...|\n|116| 0.5334251467105187|fa17c59f6eaef857b...|\n|117|0.06447333000037836|54d37471df5d8bbcf...|\n|118|0.19426472258759375|dcbeb20ec22b96d73...|\n|119| 0.2628801474534338|39236784e33cc3ce7...|\n+---+-------------------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "randomDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomAugmentedDF = randomDf.select(\"*\", manualSplitPythonUDF(\"hash\").alias(\"augmented_col\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+-------------------+--------------------+--------------------+\n| id|           random_v|                hash|       augmented_col|\n+---+-------------------+--------------------+--------------------+\n|100|0.41371264720975787|6f65a37773e1a173a...|[6f65a37773, 1a17...|\n|101| 0.7311719281896606|3c5f154011aa08151...|[3c5f154011aa0815...|\n|102| 0.9031701155118229|28fc878d1f1942bd7...|[28fc878d1f1942bd...|\n|103|0.09430205113458567|9c54ebb1c11c92d4c...|[9c54, bb1c11c92d...|\n|104|0.38340505276222947|46500976da0a25029...|[46500976da0a2502...|\n|105| 0.5569246135523511|9d8e41374ef6637e2...|[9d8, 41374, f663...|\n|106| 0.4977441406613893|29d9fa515d557568a...|[29d9fa515d557568...|\n|107| 0.2076666106201438|672da7735c57d6707...|[672da7735c57d670...|\n|108| 0.9571919406508957|e6fb1c43359cff2d7...|[, 6fb1c43359cff2...|\n|109| 0.7429395461204413|8e3f2b2dfae9a25aa...|[8, 3f2b2dfa, 9a2...|\n|110| 0.3383362304807752|9b6f99aeded8d7460...|[9b6f99a, d, d8d7...|\n|111| 0.6701724731609291|455139cdd2f5c2459...|[455139cdd2f5c245...|\n|112| 0.6417696089901257|38ee7efbe6c15078b...|[38, , 7, fb, 6c1...|\n|113| 0.7241109765059401|d81a35b183bf54307...|[d81a35b183bf5430...|\n|114|0.34089575652338666|1bd6062a7dec0b5a8...|[1bd6062a7d, c0b5...|\n|115|0.24856531779931312|9b691311770dcc119...|[9b691311770dcc11...|\n|116| 0.5334251467105187|fa17c59f6eaef857b...|[fa17c59f6, a, f8...|\n|117|0.06447333000037836|54d37471df5d8bbcf...|[54d37471df5d8bbc...|\n|118|0.19426472258759375|dcbeb20ec22b96d73...|[dcb, b20, c22b96...|\n|119| 0.2628801474534338|39236784e33cc3ce7...|[39236784, 33cc3c...|\n+---+-------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "randomAugmentedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip(ipstring):\n",
    "\n",
    "    A,B,C,D = [int(i) for i in ipstring.split('.')]\n",
    "\n",
    "    return A*256**3 + B*256**2 + C*256 + D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "IPConvertUDF = spark.udf.register(\"IPConvertUDF\", ip, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPDF = spark.createDataFrame([[\"123.123.123.123\"], [\"1.2.3.4\"], [\"127.0.0.0\"]], ['ip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---------------+----------+\n|             ip|  parsedIP|\n+---------------+----------+\n|123.123.123.123|2071690107|\n|        1.2.3.4|  16909060|\n|      127.0.0.0|2130706432|\n+---------------+----------+\n\n"
    }
   ],
   "source": [
    "IPDFWithParsedIP = IPDF.withColumn(\"parsedIP\", IPConvertUDF(\"ip\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_add(x,y):\n",
    "\n",
    "    return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "manualAddPythonUDF = spark.udf.register(\"manualAddSQLUDF\", manual_add, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "integerDF = (spark.createDataFrame([\n",
    "  (1, 2),\n",
    "  (3, 4),\n",
    "  (5, 6)\n",
    "], [\"col1\", \"col2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----+----+---+\n|col1|col2|sum|\n+----+----+---+\n|   1|   2|  3|\n|   3|   4|  7|\n|   5|   6| 11|\n+----+----+---+\n\n"
    }
   ],
   "source": [
    "integerDF.select('*',manualAddPythonUDF(\"col1\", \"col2\").alias('sum')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType,StringType,StructType,StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathOperationsSchema = StructType([\n",
    "  StructField(\"sum\", FloatType(), True), \n",
    "  StructField(\"multiplication\", FloatType(), True), \n",
    "  StructField(\"division\", FloatType(), True) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(3.0, 2.0, 0.5)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "def manual_math(x, y):\n",
    "  return (float(x + y), float(x * y), x / float(y))\n",
    "\n",
    "manual_math(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualMathPythonUDF = spark.udf.register(\"manualMathSQLUDF\", manual_math, mathOperationsSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[col1: bigint, col2: bigint, sum: struct<sum:float,multiplication:float,division:float>]"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "integerDF.select(\"*\", manualMathPythonUDF(\"col1\", \"col2\").alias(\"sum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf,PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def pandas_plus_one(v):\n",
    "    return v + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[id: bigint, id_transformed: double]"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "df = spark.range(0, 10 * 1000 * 1000)\n",
    "df.withColumn('id_transformed', pandas_plus_one(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}