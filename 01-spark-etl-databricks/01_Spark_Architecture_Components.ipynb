{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599131258304",
   "display_name": "Python 3.6.9 64-bit ('sparkit': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neecessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max,desc\n",
    "#from pyspark.sql.session import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('architecture').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangedf = spark.range(1000).toDF('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "divrange = rangedf.where(\"number % 2 = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "500"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "divrange.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Row(sum(id)=494450)]\n"
    }
   ],
   "source": [
    "# one liner \n",
    "print(spark.range(1000).where('id > 100').selectExpr(\"sum(id)\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End to End example using Flight data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FlightData2015 = spark.\\\n",
    "                    read.option(\"inferSchema\",'true').option(\"header\",'true').\\\n",
    "                    csv('/home/amogh/Documents/spark certification/Spark-The-Definitive-Guide-master/data//flight-data/csv/2015-summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "FlightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "== Physical Plan ==\n*(2) Sort [count#48 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(count#48 ASC NULLS FIRST, 200)\n   +- *(1) FileScan csv [DEST_COUNTRY_NAME#46,ORIGIN_COUNTRY_NAME#47,count#48] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/amogh/Documents/spark certification/Spark-The-Definitive-Guide-maste..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
    }
   ],
   "source": [
    "# lineage \n",
    "FlightData2015.sort('count').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark creates 200 partition by default in the above lineage we can see that, but we dont need that many partitions, lets change the partition \n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions',\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "== Physical Plan ==\n*(2) Sort [count#48 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(count#48 ASC NULLS FIRST, 5)\n   +- *(1) FileScan csv [DEST_COUNTRY_NAME#46,ORIGIN_COUNTRY_NAME#47,count#48] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/amogh/Documents/spark certification/Spark-The-Definitive-Guide-maste..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
    }
   ],
   "source": [
    "FlightData2015.sort('count').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FlightData2015.createOrReplaceTempView('flight_data_2015')\n",
    "\n",
    "sqlway = spark.sql(\"\"\"\n",
    "select DEST_COUNTRY_NAME, count(1) from flight_data_2015 group by DEST_COUNTRY_NAME\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------------------+--------+\n|   DEST_COUNTRY_NAME|count(1)|\n+--------------------+--------+\n|             Moldova|       1|\n|             Bolivia|       1|\n|             Algeria|       1|\n|Turks and Caicos ...|       1|\n|            Pakistan|       1|\n+--------------------+--------+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "sqlway.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#46], functions=[count(1)])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#46, 5)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#46], functions=[partial_count(1)])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#46] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/amogh/Documents/spark certification/Spark-The-Definitive-Guide-maste..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
    }
   ],
   "source": [
    "sqlway.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeway = FlightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#46], functions=[count(1)])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#46, 5)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#46], functions=[partial_count(1)])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#46] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/amogh/Documents/spark certification/Spark-The-Definitive-Guide-maste..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
    }
   ],
   "source": [
    "dataframeway.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both sql and dataframe are same logical plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get some intersting stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[DEST_COUNTRY_NAME: string, destination_total: bigint]"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(max(count)=370002)]"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "FlightData2015.select(max('count')).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n"
    }
   ],
   "source": [
    "FlightData2015.groupBy(\"DEST_COUNTRY_NAME\").\\\n",
    "    sum(\"count\").\\\n",
    "    withColumnRenamed(\"sum(count)\",\"destination_total\").\\\n",
    "        sort(desc(\"destination_total\")).\\\n",
    "        limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'172.17.0.1'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "spark.conf.get('spark.driver.host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'36889'"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "spark.conf.get('spark.driver.port')"
   ]
  }
 ]
}