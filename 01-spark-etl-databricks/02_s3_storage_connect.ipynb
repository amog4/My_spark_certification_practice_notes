{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599475842247",
   "display_name": "Python 3.6.9 64-bit ('sparkit': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import neecessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('s3').config('spark.jars',\"/home/amogh/Downloads/hadoop-aws-2.7.3.jar,/home/amogh/Downloads/aws-java-sdk-1.11.179.jar\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"spark.sparkContext.hadoopConfiguration.fs.s3a.awsAccessKeyId\", )\n",
    "spark.conf.set(\"spark.sparkContext.hadoopConfiguration.fs.s3a.awsSecretAccessKey\", \n",
    ")\n",
    "spark.conf.set(\"spark.sparkContext.hadoopConfiguration.fs.s3a.endpoint\", \n",
    "'ap-south-1')\n",
    "spark.conf.set(\"spark.sparkContext.hadoopConfiguration.fs.s3a.impl\", \n",
    "'org.apache.hadoop.fs.s3a.S3AFileSystem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "FlightData2015 = spark.\\\n",
    "                    read.option(\"inferSchema\",'true').option(\"header\",'true').\\\n",
    "                    csv('/home/amogh/Documents/spark certification/Spark-The-Definitive-Guide-master/data//flight-data/csv/2015-summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For databricks following to be used \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ACCESS_KEY = dbutils.secrets.get(scope = \"aws\", key = \"aws-access-key\")\n",
    "SECRET_KEY = dbutils.secrets.get(scope = \"aws\", key = \"aws-secret-key\")\n",
    "ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "AWS_BUCKET_NAME = \"<aws-bucket-name>\"\n",
    "MOUNT_NAME = \"<mount-name>\"\n",
    "\n",
    "dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n",
    "display(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}